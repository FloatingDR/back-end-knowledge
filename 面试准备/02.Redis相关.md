1.高可用搭建

2.缓存问题

3.AOF BOF

## 1. 缓存穿透

​       ~~在项目中，我们有一个场景是缉查布控，就是有一些违法犯罪、或者危险货物的车，需要跟踪它们的轨迹并进行抓捕。我们有一个黑名单库，用来匹配路上行驶的车辆，根据车辆的车牌和车辆类型唯一确定。由于路上的过车数据量很大，大概每秒有5000条数据，每一条数据都需要与黑名单进行匹配，看他是否在黑名单里面，如果在的话要进行报警。但是后面有个模糊匹配的功能，类似于在某个时间段，某个卡口布控某个类型的车辆这样子。由于对比条件比较多，而且一辆车经过每个卡口都需要对比一次，所以我们把车辆和对应的匹配结果缓存到 redis 中，并设置了一个超时时间（1小时），这样比较过一次的数据就不需要进行下次比较，直接就可以拿到比较结果。但是在实际应用中，我们发现在上下班早高峰的时候，系统查询数据很慢，数据的 IO 非常高（数据库监控软件），然后我们就查看了数据库的慢查询日志，发现很多超过 10秒的sql查询，比较多的都是进行布控比配的 sql~~

缓存穿透是指查询一个数据库一定不存在的数据，正常使用缓存的流程应该是：

1. 传进来 id 参数
2. 先到 redis 缓存中查询对应的键
3. 如果缓存中没有对应的 value
4. 就从数据库里查
5. 从数据库里面查到数据放到 redis 缓存中

在智慧路口的项目中，我们有一个场景是要转换雷达设备数据，给数据添加一些额外信息，这个时候我们需要到雷达设备表中根据设备编号查询设备信息。每个雷达设备 100 毫秒发 10到 20 条，一个路口至少有 4 个雷达设备，这样光一个路口每秒就有 400 到 800 条数据，在我们的项目中有 5 个路口，这样每秒就有 2000 到 4000条数据，考虑到数据量很大，我们使用了 redis 做缓存。每次查询雷达信息都从 redis 获取，redis 中如果没有的话再去数据库查询，查到后返回的结果缓存到 redis 中。但是有一次周末，设备厂商升级固件，把原有的设备编号都初始化为了 0。发生了缓存穿透，~~发现问题后，我们把服务都停掉，~~然后联系了硬件厂商进行修复。为了防止以后发生类似的情况，我们做了一个操作就是如果查到的雷达信息是空的，那我们也要把数据加入缓存中，也就是加一个空缓存，这个空值虽然没有实际的意义，还占用空间，但是能抵挡大量的穿透请求。其实还有另外一种方案能防止缓存穿透，比如说利用布隆过滤器，但是在我们的业务场景中，不可能出现那么多的不存在的雷达设备编号，所以这个方案能够完全满足现有的场景。



等我们周一查看的时候，发现我们的很多应用程序奔溃掉了。报错连不上数据库。

我们连上数据库，发现磁盘爆满了，然后使用 du 命令一步步查看，发现是 mysql binlog 占用了很大一部分，甚至比



## 2. Redis 实现分布式锁

## 3. 主从压力测试

```
redis-benchmark [-h <host>] [-p <port>] [-c <clients>] [-n <requests]> [-k <boolean>]

redis-benchmark -h 127.0.0.1 -p 6379 -c 50 -n 100000 -d 2
```

- 并发数：50 个。Number of parallel connections (default 50)
- 请求总数：共 10 W 个请求。Total number of requests (default 100000)
- 每个GET/SET 命令的字节数：2 Bytes。Data size of SET/GET value in bytes (default 2)

测试结果：

```
====== PING_INLINE ======
  100000 requests completed in 1.07 seconds
  50 parallel clients
  2 bytes payload
  keep alive: 1

99.37% <= 1 milliseconds
99.95% <= 2 milliseconds
100.00% <= 2 milliseconds
93808.63 requests per second

====== PING_BULK ======
  100000 requests completed in 1.12 seconds
  50 parallel clients
  2 bytes payload
  keep alive: 1

99.23% <= 1 milliseconds
99.98% <= 2 milliseconds
100.00% <= 2 milliseconds
89686.10 requests per second

====== SET ======
  100000 requests completed in 1.14 seconds
  50 parallel clients
  2 bytes payload
  keep alive: 1

98.16% <= 1 milliseconds
99.72% <= 2 milliseconds
99.90% <= 3 milliseconds
99.96% <= 4 milliseconds
100.00% <= 4 milliseconds
87950.75 requests per second

====== GET ======
  100000 requests completed in 1.07 seconds
  50 parallel clients
  2 bytes payload
  keep alive: 1

99.58% <= 1 milliseconds
99.95% <= 2 milliseconds
100.00% <= 3 milliseconds
100.00% <= 3 milliseconds
93283.58 requests per second

====== INCR ======
  100000 requests completed in 1.03 seconds
  50 parallel clients
  2 bytes payload
  keep alive: 1

99.01% <= 1 milliseconds
99.95% <= 2 milliseconds
99.98% <= 3 milliseconds
100.00% <= 4 milliseconds
100.00% <= 4 milliseconds
96993.21 requests per second

====== LPUSH ======
  100000 requests completed in 1.05 seconds
  50 parallel clients
  2 bytes payload
  keep alive: 1

98.48% <= 1 milliseconds
99.95% <= 2 milliseconds
99.99% <= 3 milliseconds
100.00% <= 3 milliseconds
95510.98 requests per second

====== RPUSH ======
  100000 requests completed in 1.09 seconds
  50 parallel clients
  2 bytes payload
  keep alive: 1

98.61% <= 1 milliseconds
99.98% <= 2 milliseconds
100.00% <= 2 milliseconds
91996.32 requests per second

====== LPOP ======
  100000 requests completed in 1.12 seconds
  50 parallel clients
  2 bytes payload
  keep alive: 1

98.11% <= 1 milliseconds
99.94% <= 2 milliseconds
100.00% <= 2 milliseconds
89365.51 requests per second

====== RPOP ======
  100000 requests completed in 1.11 seconds
  50 parallel clients
  2 bytes payload
  keep alive: 1

98.59% <= 1 milliseconds
99.89% <= 2 milliseconds
99.97% <= 3 milliseconds
99.97% <= 4 milliseconds
100.00% <= 4 milliseconds
90415.91 requests per second

====== SADD ======
  100000 requests completed in 1.06 seconds
  50 parallel clients
  2 bytes payload
  keep alive: 1

99.65% <= 1 milliseconds
100.00% <= 1 milliseconds
94339.62 requests per second

====== HSET ======
  100000 requests completed in 1.13 seconds
  50 parallel clients
  2 bytes payload
  keep alive: 1

97.41% <= 1 milliseconds
99.83% <= 2 milliseconds
99.93% <= 3 milliseconds
99.99% <= 4 milliseconds
100.00% <= 4 milliseconds
88495.58 requests per second

====== SPOP ======
  100000 requests completed in 1.12 seconds
  50 parallel clients
  2 bytes payload
  keep alive: 1

98.84% <= 1 milliseconds
99.96% <= 2 milliseconds
99.98% <= 3 milliseconds
100.00% <= 4 milliseconds
100.00% <= 4 milliseconds
89365.51 requests per second

====== LPUSH (needed to benchmark LRANGE) ======
  100000 requests completed in 1.16 seconds
  50 parallel clients
  2 bytes payload
  keep alive: 1

97.37% <= 1 milliseconds
99.88% <= 2 milliseconds
99.99% <= 3 milliseconds
100.00% <= 3 milliseconds
86281.27 requests per second

====== LRANGE_100 (first 100 elements) ======
  100000 requests completed in 2.46 seconds
  50 parallel clients
  2 bytes payload
  keep alive: 1

89.95% <= 1 milliseconds
98.60% <= 2 milliseconds
99.78% <= 3 milliseconds
99.92% <= 4 milliseconds
99.96% <= 5 milliseconds
99.97% <= 6 milliseconds
99.99% <= 7 milliseconds
100.00% <= 9 milliseconds
40683.48 requests per second

====== LRANGE_300 (first 300 elements) ======
  100000 requests completed in 5.33 seconds
  50 parallel clients
  2 bytes payload
  keep alive: 1

0.15% <= 1 milliseconds
96.73% <= 2 milliseconds
99.55% <= 3 milliseconds
99.87% <= 4 milliseconds
99.94% <= 5 milliseconds
99.98% <= 6 milliseconds
100.00% <= 7 milliseconds
100.00% <= 7 milliseconds
18765.25 requests per second

====== LRANGE_500 (first 450 elements) ======
  100000 requests completed in 7.39 seconds
  50 parallel clients
  2 bytes payload
  keep alive: 1

0.06% <= 1 milliseconds
74.48% <= 2 milliseconds
97.75% <= 3 milliseconds
99.38% <= 4 milliseconds
99.79% <= 5 milliseconds
99.89% <= 6 milliseconds
99.96% <= 7 milliseconds
100.00% <= 8 milliseconds
100.00% <= 8 milliseconds
13522.65 requests per second

====== LRANGE_600 (first 600 elements) ======
  100000 requests completed in 9.48 seconds
  50 parallel clients
  2 bytes payload
  keep alive: 1

0.01% <= 1 milliseconds
1.15% <= 2 milliseconds
93.16% <= 3 milliseconds
98.71% <= 4 milliseconds
99.58% <= 5 milliseconds
99.81% <= 6 milliseconds
99.91% <= 7 milliseconds
99.95% <= 8 milliseconds
99.97% <= 9 milliseconds
99.99% <= 10 milliseconds
100.00% <= 10 milliseconds
10544.07 requests per second

====== MSET (10 keys) ======
  100000 requests completed in 1.53 seconds
  50 parallel clients
  2 bytes payload
  keep alive: 1

90.74% <= 1 milliseconds
99.42% <= 2 milliseconds
99.98% <= 3 milliseconds
100.00% <= 3 milliseconds
65359.48 requests per second
```

**4. Redis 的 I/O 模型**

Redis 所谓的单线程并不是指 Redis 进程只有一个线程，而是说，Redis 的 I/O 和 请求处理用的是一个线程。

Redis单线程处理IO请求性能瓶颈主要包括 2 个方面：

- 任意一个请求在server中一旦发生耗时，都会影响整个server的性能，也就是说后面的请求都要等前面这个耗时请求处理完成，自己才能被处理到。

  > 耗时的操作包括以下几种： 
  >
  > 1. 操作bigkey：写入一个bigkey在分配内存时需要消耗更多的时间，同样，删除bigkey释放内存同样会产生耗时；
  > 2. 使用复杂度过高的命令：例如SORT/SUNION/ZUNIONSTORE，或者O(N)命令，但是N很大，例如 lrange key 0 -1一次查询全量数据，根据我的测试，使用 LRANGE 0 600 处理完 10万请求需要将近 9.5 秒，这对于我们印象中的 Redis 来说简直天差地别 ； 
  > 3. 大量key集中过期：Redis的过期机制也是在主线程中执行的，大量key集中过期会导致处理一个请求时，耗时都在删除过期key，耗时变长；
  > 4. 淘汰策略：淘汰策略也是在主线程执行的，当内存超过Redis内存上限后，每次写入都需要淘汰一些key，也会造成耗时变长；
  > 5. AOF刷盘开启always机制：每次写入都需要把这个操作刷到磁盘，写磁盘的速度远比写内存慢，会拖慢Redis的性能； 
  > 6. 主从全量同步生成RDB：虽然采用fork子进程生成数据快照，但fork这一瞬间也是会阻塞整个线程的，实例越大，阻塞时间越久；

- 并发量非常大时，单线程读写客户端IO数据存在性能瓶颈，虽然采用IO多路复用机制，但是读写客户端数据依旧是同步IO，只能单线程依次读取客户端的数据，无法利用到CPU多核。 

针对第一个问题，一方面需要业务人员去规避，一方面Redis在4.0推出了lazy-free机制，把bigkey释放内存的耗时操作放在了异步线程中执行，降低对主线程的影响。 

针对第二个问题 在 2020 年 5 月Redis  6.0 推出了多线程，可以在高并发场景下利用CPU多核多线程读写客户端数据，进一步提升server性能，当然，只是针对客户端的读写是并行的，每个命令的真正操作依旧是单线程的。

